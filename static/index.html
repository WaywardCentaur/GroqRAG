<!DOCTYPE html>
<html>
<head>
    <title>Audio RAG Tester</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .section {
            border: 1px solid #ddd;
            padding: 20px;
            margin-bottom: 20px;
            border-radius: 5px;
        }
        .video-container {
            position: relative;
            width: 100%;
            padding-bottom: 56.25%; /* 16:9 Aspect Ratio */
            margin-bottom: 20px;
            background: #000; /* Dark background while loading */
            border-radius: 8px;
            overflow: hidden;
        }
        .video-container #player {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
        }
        .video-container iframe {
            border-radius: 8px;
        }
        .response {
            background: #f4f4f4;
            padding: 10px;
            margin-top: 10px;
            border-radius: 3px;
            white-space: pre-wrap;
        }
        button {
            background: #4CAF50;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 3px;
            cursor: pointer;
        }
        button:disabled {
            background: #cccccc;
        }
        input[type="text"] {
            width: 100%;
            padding: 8px;
            margin: 5px 0;
            box-sizing: border-box;
        }
        #transcriptionArea {
            max-height: 300px;
            overflow-y: auto;
        }
        .transcription-item {
            padding: 5px;
            border-bottom: 1px solid #eee;
        }
        #audioStatus {
            margin-top: 10px;
            font-style: italic;
        }
        .video-container {
            position: relative;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            height: 0;
            overflow: hidden;
            max-width: 100%;
            background: #000;
            margin-bottom: 20px;
        }
        .video-container #player {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
        }
        .table-container {
            margin-top: 10px;
            overflow-x: auto;
        }
        .data-table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 10px;
        }
        .data-table th, .data-table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        .data-table th {
            background-color: #f2f2f2;
            font-weight: bold;
        }
        .data-table tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        .delete-btn {
            background: #dc3545;
            color: white;
            border: none;
            padding: 4px 8px;
            border-radius: 3px;
            cursor: pointer;
            font-size: 12px;
        }
        .delete-btn:hover {
            background: #c82333;
        }
        .delete-btn:disabled {
            background: #cccccc;
            cursor: not-allowed;
        }
    </style>
</head>
<body>
    <h1>Audio RAG Tester</h1>

    <!-- Document Upload Section -->
    <div class="section">
        <h2>Upload Document</h2>
        <input type="file" id="documentInput" accept=".pdf,.txt,.docx">
        <button onclick="uploadDocument()">Upload</button>
        <div id="uploadResponse" class="response"></div>
    </div>

    <!-- Query Section -->
    <div class="section">
        <h2>Query RAG</h2>
        <input type="text" id="queryInput" placeholder="Enter your question here">
        <button onclick="sendQuery()">Ask</button>
        <div id="queryResponse" class="response"></div>
    </div>

    <!-- Documents Table Section -->
    <div class="section">
        <h2>Documents</h2>
        <button onclick="loadDocuments()">Refresh Documents</button>
        <div id="documentsTable" class="table-container"></div>
    </div>

    <!-- Transcriptions Table Section -->
    <div class="section">
        <h2>Saved Transcriptions</h2>
        <button onclick="loadTranscriptions()">Refresh Transcriptions</button>
        <div id="transcriptionsTable" class="table-container"></div>
    </div>

    <!-- Video and Audio Section -->
    <div class="section">
        <h2>Audio Transcription Processing</h2>
        <div class="video-container">
            <!-- YouTube video will be loaded here -->
            <div id="player"></div>
        </div>
        <div class="audio-container">
            <p>Click Start to begin processing the audio and saving transcriptions as context.</p>
            <button id="audioButton" onclick="toggleAudio()">Start Processing</button>
            <div id="audioStatus">Ready to process audio and save transcriptions</div>
            <div id="transcriptionArea"></div>
            <label for="transcriptionBox"><strong>Latest Transcription:</strong></label>
            <textarea id="transcriptionBox" rows="4" style="width:100%;margin-top:8px;" readonly></textarea>
        </div>
    </div>

    <!-- YouTube IFrame API -->
    <script>
        // Helper function to convert ArrayBuffer to Base64
        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.byteLength; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }

        // Load YouTube IFrame API dynamically
        var tag = document.createElement('script');
        tag.src = "https://www.youtube.com/iframe_api";
        var firstScriptTag = document.getElementsByTagName('script')[0];
        firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

        let ws = null;
        let isRecording = false;
        let audioContext = null;
        let player = null;
        
        // YouTube player initialization
        function onYouTubeIframeAPIReady() {
            console.log('YouTube IFrame API Ready');
            player = new YT.Player('player', {
                height: '360',
                width: '640',
                videoId: 'eXdVDhOGqoE',  // Sam Altman TED Talk 2023
                playerVars: {
                    'autoplay': 0,
                    'playsinline': 1,
                    'controls': 1,
                    'start': 30,  // Start 30 seconds in to skip the intro
                    'rel': 0,     // Don't show related videos
                    'modestbranding': 1,  // Show minimal YouTube branding
                    'enablejsapi': 1,     // Enable JavaScript API
                    'origin': window.location.origin  // Set origin for security
                },
                events: {
                    'onReady': onPlayerReady,
                    'onStateChange': onPlayerStateChange
                }
            });
        }

        function onPlayerReady(event) {
            console.log('Player ready');
            const audioStatus = document.getElementById('audioStatus');
            audioStatus.textContent = 'Audio loaded and ready to process';
            // Ensure the player is visible
            event.target.setSize(640, 360);
        }

        function onPlayerStateChange(event) {
            console.log('Player state changed:', event.data);
            const audioStatus = document.getElementById('audioStatus');
            switch(event.data) {
                case YT.PlayerState.PLAYING:
                    audioStatus.textContent = 'Audio is playing - transcriptions will be saved as context';
                    break;
                case YT.PlayerState.PAUSED:
                    audioStatus.textContent = 'Audio is paused';
                    break;
                case YT.PlayerState.ENDED:
                    audioStatus.textContent = 'Audio ended';
                    break;
            }
        }

        async function uploadDocument() {
            const fileInput = document.getElementById('documentInput');
            const responseDiv = document.getElementById('uploadResponse');
            
            if (!fileInput.files.length) {
                responseDiv.textContent = 'Please select a file first';
                return;
            }
            
            const formData = new FormData();
            formData.append('file', fileInput.files[0]);
            
            try {
                const response = await fetch('/documents', {
                    method: 'POST',
                    body: formData
                });
                const data = await response.json();
                responseDiv.textContent = data.message;
                
                // Refresh documents table after successful upload
                if (response.ok) {
                    loadDocuments();
                }
            } catch (error) {
                responseDiv.textContent = 'Error uploading document: ' + error.message;
            }
        }

        async function sendQuery() {
            const queryInput = document.getElementById('queryInput');
            const responseDiv = document.getElementById('queryResponse');
            
            if (!queryInput.value.trim()) {
                responseDiv.textContent = 'Please enter a question';
                return;
            }
            
            try {
                const response = await fetch('/query', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ text: queryInput.value })
                });
                const data = await response.text();
                responseDiv.textContent = data;
            } catch (error) {
                responseDiv.textContent = 'Error querying: ' + error.message;
            }
        }

        async function loadTranscriptions() {
            const transcriptionsDiv = document.getElementById('transcriptionsTable');
            try {
                const response = await fetch('/transcriptions');
                const data = await response.json();
                
                if (data.transcriptions && data.transcriptions.length > 0) {
                    let html = `
                        <table class="data-table">
                            <thead>
                                <tr>
                                    <th>ID</th>
                                    <th>Source</th>
                                    <th>Timestamp</th>
                                    <th>Chunks</th>
                                    <th>Actions</th>
                                </tr>
                            </thead>
                            <tbody>
                    `;
                    
                    data.transcriptions.forEach((transcription) => {
                        const shortId = transcription.transcription_id.substring(0, 8) + '...';
                        const formattedDate = new Date(transcription.timestamp).toLocaleString();
                        html += `
                            <tr>
                                <td title="${transcription.transcription_id}">${shortId}</td>
                                <td>${transcription.source}</td>
                                <td>${formattedDate}</td>
                                <td>${transcription.chunk_count}</td>
                                <td>
                                    <button class="delete-btn" onclick="deleteTranscription('${transcription.transcription_id}')">
                                        Delete
                                    </button>
                                </td>
                            </tr>
                        `;
                    });
                    
                    html += `
                            </tbody>
                        </table>
                    `;
                    transcriptionsDiv.innerHTML = html;
                } else {
                    transcriptionsDiv.innerHTML = '<p>No transcriptions found. Start processing audio to save transcriptions as context.</p>';
                }
            } catch (error) {
                transcriptionsDiv.innerHTML = '<p>Error loading transcriptions: ' + error.message + '</p>';
            }
        }

        async function loadDocuments() {
            const documentsDiv = document.getElementById('documentsTable');
            try {
                const response = await fetch('/documents');
                const data = await response.json();
                
                if (data.documents && data.documents.length > 0) {
                    let html = `
                        <table class="data-table">
                            <thead>
                                <tr>
                                    <th>ID</th>
                                    <th>Filename</th>
                                    <th>Chunks</th>
                                    <th>Actions</th>
                                </tr>
                            </thead>
                            <tbody>
                    `;
                    
                    data.documents.forEach((document) => {
                        const shortId = document.document_id.substring(0, 8) + '...';
                        html += `
                            <tr>
                                <td title="${document.document_id}">${shortId}</td>
                                <td>${document.filename}</td>
                                <td>${document.chunk_count}</td>
                                <td>
                                    <button class="delete-btn" onclick="deleteDocument('${document.document_id}')">
                                        Delete
                                    </button>
                                </td>
                            </tr>
                        `;
                    });
                    
                    html += `
                            </tbody>
                        </table>
                    `;
                    documentsDiv.innerHTML = html;
                } else {
                    documentsDiv.innerHTML = '<p>No documents found. Upload documents to see them here.</p>';
                }
            } catch (error) {
                documentsDiv.innerHTML = '<p>Error loading documents: ' + error.message + '</p>';
            }
        }

        async function deleteTranscription(transcriptionId) {
            if (!confirm('Are you sure you want to delete this transcription? This action cannot be undone.')) {
                return;
            }

            try {
                const response = await fetch(`/transcriptions/${transcriptionId}`, {
                    method: 'DELETE'
                });

                if (response.ok) {
                    alert('Transcription deleted successfully');
                    loadTranscriptions(); // Refresh the table
                } else {
                    const error = await response.text();
                    alert('Error deleting transcription: ' + error);
                }
            } catch (error) {
                alert('Error deleting transcription: ' + error.message);
            }
        }

        async function deleteDocument(documentId) {
            if (!confirm('Are you sure you want to delete this document? This action cannot be undone.')) {
                return;
            }

            try {
                const response = await fetch(`/documents/${documentId}`, {
                    method: 'DELETE'
                });

                if (response.ok) {
                    alert('Document deleted successfully');
                    loadDocuments(); // Refresh the table
                } else {
                    const error = await response.text();
                    alert('Error deleting document: ' + error);
                }
            } catch (error) {
                alert('Error deleting document: ' + error.message);
            }
        }

        async function toggleAudio() {
            const button = document.getElementById('audioButton');
            const statusDiv = document.getElementById('audioStatus');

            if (!isRecording) {
                try {
                    // Initialize WebSocket first
                    ws = new WebSocket(`ws://${window.location.host}/audio`);
                    console.log('Creating WebSocket connection...');
                    
                    // Set a connection timeout
                    const connectionTimeout = setTimeout(() => {
                        if (ws.readyState !== WebSocket.OPEN) {
                            console.error('WebSocket connection timeout');
                            ws.close();
                            throw new Error('WebSocket connection timeout');
                        }
                    }, 5000);
                    
                    ws.onopen = () => {
                        clearTimeout(connectionTimeout);
                        console.log('WebSocket connected successfully');
                        statusDiv.textContent = 'Connected, starting audio processing...';
                        
                        // Set up ping/pong to keep connection alive
                        const pingInterval = setInterval(() => {
                            if (ws.readyState === WebSocket.OPEN) {
                                ws.send(JSON.stringify({ type: 'ping' }));
                            } else {
                                clearInterval(pingInterval);
                            }
                        }, 30000);
                        
                        // Clean up ping interval when connection closes
                        ws.addEventListener('close', () => clearInterval(pingInterval));
                    };

                    ws.onmessage = (event) => {
                        try {
                            const data = JSON.parse(event.data);
                            console.log('Received WebSocket message:', data);
                            
                            if (data.type === 'transcription') {
                                const transcriptionDiv = document.getElementById('transcriptionArea');
                                const item = document.createElement('div');
                                item.className = 'transcription-item';
                                item.textContent = `[${data.timestamp || 'now'}] ${data.text}`;
                                transcriptionDiv.appendChild(item);
                                transcriptionDiv.scrollTop = transcriptionDiv.scrollHeight;
                                statusDiv.textContent = 'Actively transcribing audio and saving as context...';
                                // Show latest transcription in the reserved textbox
                                const transcriptionBox = document.getElementById('transcriptionBox');
                                if (transcriptionBox) {
                                    transcriptionBox.value = data.text;
                                }
                            } else if (data.type === 'error') {
                                console.error('Server error:', data.message);
                                statusDiv.textContent = 'Server error: ' + data.message;
                            } else {
                                console.log('Unknown message type:', data.type);
                            }
                        } catch (error) {
                            console.error('Error handling WebSocket message:', error);
                            statusDiv.textContent = 'Error processing server message';
                        }
                    };

                    ws.onerror = (error) => {
                        console.error('WebSocket error:', error);
                        statusDiv.textContent = 'WebSocket error occurred';
                    };

                    ws.onclose = () => {
                        statusDiv.textContent = 'Connection closed';
                        isRecording = false;
                        button.textContent = 'Start Processing';
                        if (player) {
                            player.pauseVideo();
                        }
                    };

                    try {
                        // Request system audio capture first
                        console.log('Requesting audio capture...');
                        const stream = await navigator.mediaDevices.getDisplayMedia({
                            video: {
                                displaySurface: 'browser',  // Prefer capturing browser tab
                            },
                            audio: {
                                echoCancellation: true,
                                noiseSuppression: true,
                                autoGainControl: true,
                            },
                            preferCurrentTab: true,
                            systemAudio: 'include'  // Explicitly include system audio
                        });

                        // Verify we got audio tracks
                        const audioTracks = stream.getAudioTracks();
                        if (audioTracks.length === 0) {
                            throw new Error('No audio track available in the captured stream');
                        }
                        console.log('Audio capture successful:', audioTracks[0].label);

                        // Now start the audio
                        if (player) {
                            console.log('Starting audio playback...');
                            player.playVideo();
                        }

                        // Set up audio processing
                        audioContext = new (window.AudioContext || window.webkitAudioContext)();
                        await audioContext.audioWorklet.addModule('data:text/javascript;base64,' + btoa(`
                            class AudioProcessor extends AudioWorkletProcessor {
                                constructor() {
                                    super();
                                    this.targetSampleRate = 16000;  // Target sample rate for Whisper
                                    this.bufferSize = 4096;        // Buffer size for processing
                                    this.buffer = new Float32Array(this.bufferSize);
                                    this.bufferedSamples = 0;
                                    // Calculate downsampling ratio based on actual sample rate
                                    this.downsampleRatio = Math.round(sampleRate / this.targetSampleRate);
                                    // Simple anti-aliasing filter
                                    this.filterCoef = new Float32Array(this.downsampleRatio).fill(1 / this.downsampleRatio);
                                }
                                
                                process(inputs, outputs, parameters) {
                                    const input = inputs[0][0];
                                    if (!input) return true;
                                    
                                    // Process input samples
                                    for (let i = 0; i < input.length; i++) {
                                        // Apply anti-aliasing filter and downsample
                                        if (i % this.downsampleRatio === 0) {
                                            // Apply filter
                                            let sample = 0;
                                            for (let j = 0; j < this.downsampleRatio; j++) {
                                                if (i + j < input.length) {
                                                    sample += input[i + j] * this.filterCoef[j];
                                                }
                                            }
                                            
                                            this.buffer[this.bufferedSamples++] = sample;
                                            
                                            if (this.bufferedSamples >= this.bufferSize) {
                                                // Convert to 16-bit PCM with proper scaling
                                                const samples = new Int16Array(this.bufferSize);
                                                for (let j = 0; j < this.bufferSize; j++) {
                                                    // Scale to 16-bit range and clip
                                                    const scaled = Math.max(-1, Math.min(1, this.buffer[j])) * 0x7FFF;
                                                    samples[j] = Math.round(scaled);
                                                }
                                                
                                                // Send the buffer
                                                this.port.postMessage(samples.buffer, [samples.buffer]);
                                                this.bufferedSamples = 0;
                                            }
                                        }
                                    }
                                    return true;
                                }
                            }
                            registerProcessor('audio-processor', AudioProcessor);
                        `));

                        const source = audioContext.createMediaStreamSource(stream);
                        const node = new AudioWorkletNode(audioContext, 'audio-processor');
                        
                        // Create a buffer for accumulating audio data
                        let accumulatedData = new Int16Array(0);
                        
                        node.port.onmessage = (e) => {
                            if (ws && ws.readyState === WebSocket.OPEN && isRecording) {
                                try {
                                    // Convert the ArrayBuffer to Int16Array
                                    const newData = new Int16Array(e.data);
                                    
                                    // Accumulate data until we have enough samples
                                    const totalLength = accumulatedData.length + newData.length;
                                    const combinedData = new Int16Array(totalLength);
                                    combinedData.set(accumulatedData);
                                    combinedData.set(newData, accumulatedData.length);
                                    accumulatedData = combinedData;
                                    
                                    // If we have enough data (about 1 second), send it
                                    if (accumulatedData.length >= audioContext.sampleRate) {
                                        console.log('Preparing audio chunk, samples:', accumulatedData.length);
                                        
                                        // Convert Int16Array to Base64 string properly
                                        const buffer = accumulatedData.buffer;
                                        const base64data = arrayBufferToBase64(buffer);
                                        
                                        // Create a JSON message with audio metadata and base64 encoded audio data
                                        const audioData = {
                                            type: 'audio_data',
                                            format: 'pcm_s16le',  // 16-bit signed little-endian PCM
                                            channels: 1,          // mono
                                            sample_rate: 16000,   // 16kHz
                                            samples: accumulatedData.length,
                                            data: base64data
                                        };
                                        
                                        // Send as a JSON string
                                        ws.send(JSON.stringify(audioData));
                                        
                                        console.log('Sent audio chunk with metadata, base64 length:', base64data.length);
                                        
                                        // Reset the accumulated data
                                        accumulatedData = new Int16Array(0);
                                    }
                                } catch (error) {
                                    console.error('Error processing audio chunk:', error);
                                }
                            }
                        };

                        // Only connect source to node, not to destination (to avoid echo)
                        source.connect(node);

                        // Start playing the audio
                        if (player) {
                            player.playVideo();
                        }

                        isRecording = true;
                        button.textContent = 'Stop Processing';
                        statusDiv.textContent = 'Processing audio and saving transcriptions...';

                        // Clean up when the stream ends
                        const tracks = stream.getTracks();
                        tracks.forEach(track => {
                            track.onended = () => {
                                if (ws) ws.close();
                                if (audioContext) {
                                    audioContext.close();
                                    audioContext = null;
                                }
                                if (player) player.pauseVideo();
                                isRecording = false;
                                button.textContent = 'Start Processing';
                                statusDiv.textContent = 'Stopped';
                                
                                // Clean up remaining tracks
                                tracks.forEach(t => t.stop());
                            };
                        });

                    } catch (error) {
                        console.error('Error accessing system audio:', error);
                        throw new Error('Please select a window/tab with the audio content and enable audio sharing');
                    }

                } catch (error) {
                    console.error('Error starting audio processing:', error);
                    statusDiv.textContent = 'Error: ' + error.message;
                    if (ws) ws.close();
                }
            } else {
                // Stop processing
                console.log('Starting cleanup...');
                isRecording = false;
                
                // Clean up WebSocket
                if (ws) {
                    try {
                        if (ws.readyState === WebSocket.OPEN) {
                            ws.send(JSON.stringify({ type: 'stop' }));
                            await new Promise(resolve => {
                                const closeTimeout = setTimeout(() => {
                                    ws.close();
                                    resolve();
                                }, 1000);
                                ws.onclose = () => {
                                    clearTimeout(closeTimeout);
                                    resolve();
                                };
                            });
                        }
                    } catch (error) {
                        console.error('Error closing WebSocket:', error);
                    } finally {
                        ws = null;
                    }
                }
                
                // Clean up audio context
                if (audioContext) {
                    try {
                        await audioContext.close();
                    } catch (error) {
                        console.error('Error closing AudioContext:', error);
                    } finally {
                        audioContext = null;
                    }
                }
                
                // Stop audio playback
                if (player) {
                    try {
                        player.pauseVideo();
                    } catch (error) {
                        console.error('Error pausing audio:', error);
                    }
                }
                
                button.textContent = 'Start Processing';
                statusDiv.textContent = 'Stopped';
            }
        }

        // Auto-load tables when page loads
        window.addEventListener('load', function() {
            loadDocuments();
            loadTranscriptions();
        });
    </script>
</body>
</html>
